{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483e634a",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb968e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a061d",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82678e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight = pd.read_csv('Airline_Delay_Cause.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4625a4b",
   "metadata": {},
   "source": [
    "### Drop Columns or Rows with evaluation sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bdcfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=flight.drop(['arr_delay'], axis=1)\n",
    "test.sample(n=10, random_state=42).to_csv(\"test_sample.csv\", index=False)\n",
    "evaluation=flight\n",
    "evaluation.sample(n=10, random_state=42).to_csv(\"eval_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621bff0d",
   "metadata": {},
   "source": [
    "### Drop Columns or Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff05bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "flight.dropna(inplace=True)\n",
    "\n",
    "# Convert categories\n",
    "cat_cols = ['carrier', 'carrier_name', 'airport', 'airport_name']\n",
    "for col in cat_cols:\n",
    "    flight[col] = flight[col].astype('category')\n",
    "\n",
    "# Convert counts to int\n",
    "flight['arr_flights'] = flight['arr_flights'].astype(int)\n",
    "flight['arr_del15'] = flight['arr_del15'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe25c6",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ade69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_regressor(df, fillna=True):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['arr_flights'] = df['arr_flights'].replace(0, np.nan)\n",
    "\n",
    "    # Normalize rates\n",
    "    df['delay_ratio'] = df['arr_del15'] / df['arr_flights']\n",
    "    df['cancellation_rate'] = df['arr_cancelled'] / df['arr_flights']\n",
    "    df['diversion_rate'] = df['arr_diverted'] / df['arr_flights']\n",
    "\n",
    "    # Total delay\n",
    "    delay_cols = ['carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "    df['total_delay'] = df[delay_cols].sum(axis=1)\n",
    "    df['total_delay'] = df['total_delay'].replace(0, np.nan)\n",
    "\n",
    "    # Delay % features\n",
    "    for col in delay_cols:\n",
    "        new_col = col.replace('_delay', '_delay_pct')\n",
    "        df[new_col] = df[col] / df['total_delay']\n",
    "    pct_cols = [col.replace('_delay', '_delay_pct') for col in delay_cols]\n",
    "    if fillna:\n",
    "        df[pct_cols] = df[pct_cols].fillna(0)\n",
    "\n",
    "    # Year-Month\n",
    "    df['year_month'] = df['year'].astype(str) + '-' + df['month'].astype(str).str.zfill(2)\n",
    "\n",
    "    # Season\n",
    "    df['season'] = df['month'].apply(lambda m: (\n",
    "        'Winter' if m in [12, 1, 2] else\n",
    "        'Spring' if m in [3, 4, 5] else\n",
    "        'Summer' if m in [6, 7, 8] else\n",
    "        'Fall'\n",
    "    ))\n",
    "\n",
    "    # Carrier total flights\n",
    "    df['carrier_total_flights'] = df.groupby('carrier', observed=False)['arr_flights'].transform('sum')\n",
    "\n",
    "    # Airport delay rate\n",
    "    airport_delay_ratio = df.groupby('airport', observed=False).apply(\n",
    "        lambda x: x['arr_del15'].sum() / x['arr_flights'].sum()\n",
    "    )\n",
    "    df['airport_delay_rate'] = df['airport'].map(airport_delay_ratio)\n",
    "\n",
    "    # Disruption flag\n",
    "    df['disrupted'] = ((df['arr_cancelled'] > 0) | (df['arr_diverted'] > 0)).astype(int)\n",
    "\n",
    "    # Risk classification\n",
    "    def classify_risk(ratio):\n",
    "        if pd.isna(ratio):\n",
    "            return np.nan\n",
    "        elif ratio <= 0.20:\n",
    "            return 0\n",
    "        elif ratio <= 0.40:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    df['delay_risk_level'] = df['delay_ratio'].apply(classify_risk)\n",
    "\n",
    "    # Mean delay per flight\n",
    "    df['mean_delay_per_flight'] = df['total_delay'] / df['arr_flights']\n",
    "    if fillna:\n",
    "        df['mean_delay_per_flight'] = df['mean_delay_per_flight'].fillna(0)\n",
    "\n",
    "    # Dominant delay cause\n",
    "    df['dominant_delay_cause'] = df[delay_cols].idxmax(axis=1)\n",
    "\n",
    "    # Monthly delay rate\n",
    "    month_delay = df.groupby('month', observed=False).apply(\n",
    "        lambda x: x['arr_del15'].sum() / x['arr_flights'].sum()\n",
    "    )\n",
    "    df['month_delay_rate'] = df['month'].map(month_delay)\n",
    "\n",
    "    # Delay pressure\n",
    "    df['carrier_vs_airport_ratio'] = df['carrier_delay_pct'] / (\n",
    "        df['weather_delay_pct'] + df['nas_delay_pct'] + 1e-6\n",
    "    )\n",
    "\n",
    "    # Seasonal-airport combo\n",
    "    df['season_airport_combo'] = df['season'].astype(str) + '_' + df['airport'].astype(str)\n",
    "    season_airport_delay = df.groupby('season_airport_combo', observed=False).apply(\n",
    "        lambda x: x['arr_del15'].sum() / x['arr_flights'].sum()\n",
    "    )\n",
    "    df['season_airport_delay_rate'] = df['season_airport_combo'].map(season_airport_delay)\n",
    "\n",
    "    # âœ… Final NaN fill ONLY for numeric columns\n",
    "    if fillna:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7666172",
   "metadata": {},
   "source": [
    "### train,validation and test split then apply feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ebbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omar yasser\\AppData\\Local\\Temp\\ipykernel_3916\\109016724.py:40: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  lambda x: x['arr_del15'].sum() / x['arr_flights'].sum()\n",
      "C:\\Users\\omar yasser\\AppData\\Local\\Temp\\ipykernel_3916\\109016724.py:40: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  lambda x: x['arr_del15'].sum() / x['arr_flights'].sum()\n",
      "C:\\Users\\omar yasser\\AppData\\Local\\Temp\\ipykernel_3916\\109016724.py:40: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  lambda x: x['arr_del15'].sum() / x['arr_flights'].sum()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ==============================================================\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# - Split BEFORE feature engineering\n",
    "# - Target: arr_delay\n",
    "# ==============================================================\n",
    "\n",
    "# STEP 1: Copy raw data before any engineered columns\n",
    "raw_df = flight.copy()\n",
    "\n",
    "# STEP 2: First split â†’ 85% train_val, 15% test\n",
    "X_raw_trainval, X_raw_test = train_test_split(raw_df, test_size=0.15, random_state=42)\n",
    "\n",
    "# STEP 3: Second split â†’ train (70%) and validation (15%)\n",
    "X_raw_train, X_raw_val = train_test_split(X_raw_trainval, test_size=0.1765, random_state=42)  # â‰ˆ 15% of total\n",
    "\n",
    "# STEP 4: Feature Engineering (apply same pipeline used in classifier)\n",
    "X_train_fe = feature_engineering_regressor(X_raw_train)\n",
    "X_val_fe   = feature_engineering_regressor(X_raw_val)\n",
    "X_test_fe  = feature_engineering_regressor(X_raw_test)\n",
    "\n",
    "# STEP 5: Extract regression target\n",
    "y_train_reg = X_train_fe['arr_delay']\n",
    "y_val_reg   = X_val_fe['arr_delay']\n",
    "y_test_reg  = X_test_fe['arr_delay']\n",
    "\n",
    "# STEP 6: Drop target from feature sets\n",
    "X_train_fe.drop(['arr_delay'], axis=1, inplace=True)\n",
    "X_val_fe.drop(['arr_delay'], axis=1, inplace=True)\n",
    "X_test_fe.drop(['arr_delay'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025877ca",
   "metadata": {},
   "source": [
    "### Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "574060d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "def preprocessing_regressor(df, fit=True, scaler=None, encoder=None):\n",
    "    \"\"\"\n",
    "    Preprocessing for Regression Task:\n",
    "    - Drops leakage-prone and irrelevant columns\n",
    "    - Encodes categorical features using OneHotEncoder\n",
    "    - Scales numerical features using StandardScaler\n",
    "    - Returns: processed_df, scaler, encoder\n",
    "    \"\"\"\n",
    "\n",
    "    # Columns that leak or are irrelevant to regression modeling\n",
    "    drop_cols = [\n",
    "        'carrier_name', 'airport_name',\n",
    "        'arr_flights', 'arr_del15',\n",
    "        'carrier_ct', 'weather_ct', 'nas_ct', 'security_ct', 'late_aircraft_ct',\n",
    "        'arr_cancelled', 'arr_diverted',\n",
    "        'arr_delay',  # âœ… this is the target\n",
    "        'carrier_delay', 'weather_delay', 'nas_delay',\n",
    "        'security_delay', 'late_aircraft_delay', 'total_delay',\n",
    "        'delay_ratio', 'high_delay_flag',\n",
    "        'delay_risk_level', 'year_month', 'season_airport_combo'\n",
    "    ]\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    # Fix dtype for any accidental categorical numerics\n",
    "    to_str_cols = ['carrier_total_flights', 'airport_delay_rate']\n",
    "    for col in to_str_cols:\n",
    "        if col in df.columns and pd.api.types.is_categorical_dtype(df[col]):\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    # Identify feature types\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    # One-hot encode categoricals\n",
    "    if fit:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded_cat = encoder.fit_transform(df[categorical_cols])\n",
    "    else:\n",
    "        encoded_cat = encoder.transform(df[categorical_cols])\n",
    "    encoded_cat_df = pd.DataFrame(encoded_cat, columns=encoder.get_feature_names_out(categorical_cols), index=df.index)\n",
    "\n",
    "    # Scale numericals\n",
    "    if fit:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_num = scaler.fit_transform(df[numeric_cols])\n",
    "    else:\n",
    "        scaled_num = scaler.transform(df[numeric_cols])\n",
    "    scaled_num_df = pd.DataFrame(scaled_num, columns=numeric_cols, index=df.index)\n",
    "\n",
    "    # Merge\n",
    "    processed_df = pd.concat([scaled_num_df, encoded_cat_df], axis=1)\n",
    "\n",
    "    # Safety fill for any unexpected NaNs\n",
    "    processed_df = processed_df.fillna(0)\n",
    "\n",
    "    return processed_df, scaler, encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1d8a9",
   "metadata": {},
   "source": [
    "### Dumbing the encoder and the scaler and applying preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03dfefc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omar yasser\\AppData\\Local\\Temp\\ipykernel_3916\\633148634.py:29: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col in df.columns and pd.api.types.is_categorical_dtype(df[col]):\n",
      "C:\\Users\\omar yasser\\AppData\\Local\\Temp\\ipykernel_3916\\633148634.py:29: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col in df.columns and pd.api.types.is_categorical_dtype(df[col]):\n",
      "C:\\Users\\omar yasser\\AppData\\Local\\Temp\\ipykernel_3916\\633148634.py:29: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if col in df.columns and pd.api.types.is_categorical_dtype(df[col]):\n"
     ]
    }
   ],
   "source": [
    "X_train_processed, scaler, encoder = preprocessing_regressor(X_train_fe, fit=True)\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save fitted encoder, scaler, and the correct feature order\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "joblib.dump(encoder, \"encoder.pkl\")\n",
    "joblib.dump(X_train_processed.columns.tolist(), \"feature_order.pkl\")\n",
    "\n",
    "\n",
    "X_val_processed, _, _ = preprocessing_regressor(X_val_fe, fit=False, scaler=scaler, encoder=encoder)\n",
    "X_test_processed, _, _ = preprocessing_regressor(X_test_fe, fit=False, scaler=scaler, encoder=encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eead78",
   "metadata": {},
   "source": [
    "### Evaluation function and ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e3d6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    median_absolute_error, mean_squared_log_error\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_regression_metrics(y_true, y_pred, label=\"Set\"):\n",
    "    \"\"\"Prints core regression metrics for a given dataset.\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    medae = median_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\nðŸ“Š {label} Set Metrics:\")\n",
    "    print(f\"   - MAE:     {mae:.2f}\")\n",
    "    print(f\"   - MedAE:   {medae:.2f}\")\n",
    "    print(f\"   - RMSE:    {rmse:.2f}\")\n",
    "    print(f\"   - RÂ²:      {r2:.4f}\")\n",
    "\n",
    "    try:\n",
    "        msle = mean_squared_log_error(np.clip(y_true, a_min=0, a_max=None),\n",
    "                                      np.clip(y_pred, a_min=0, a_max=None))\n",
    "        print(f\"   - MSLE:    {msle:.4f}\")\n",
    "    except:\n",
    "        print(f\"   - MSLE:    Skipped (negative values detected)\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def plot_regression_diagnostics(X_fe_df, y_true, y_pred, label=\"Set\"):\n",
    "    \"\"\"Draws correlation matrix and predicted-vs-actual scatter plot.\"\"\"\n",
    "    \n",
    "    # Correlation matrix\n",
    "    numeric_df = X_fe_df.select_dtypes(include=[np.number])\n",
    "    corr = numeric_df.corr()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr, cmap='coolwarm', center=0, annot=False)\n",
    "    plt.title(f\"{label} Set | Correlation Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Predicted vs actual\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, alpha=0.3)\n",
    "    plt.xlabel(\"True Arrival Delay (min)\")\n",
    "    plt.ylabel(\"Predicted Arrival Delay (min)\")\n",
    "    plt.title(f\"{label} Set | Predicted vs Actual\")\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], '--', color='red')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b990da1a",
   "metadata": {},
   "source": [
    "### optuna tuning for hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8310a339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\mid_project_raw\\flight_delay_dashboard_project\\envs\\dashboard_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-06-11 20:24:30,761] A new study created in memory with name: no-name-bc92026c-c6d2-43eb-ae0c-12f39b39a341\n",
      "[I 2025-06-11 20:24:40,295] Trial 0 finished with value: 1785.5391191928645 and parameters: {'n_estimators': 111, 'max_depth': 6, 'learning_rate': 0.09855988569133031, 'subsample': 0.7000692971979823, 'colsample_bytree': 0.9914280076289943, 'reg_alpha': 0.8971207068496274, 'reg_lambda': 0.9206149086096961}. Best is trial 0 with value: 1785.5391191928645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Best Params: {'n_estimators': 111, 'max_depth': 6, 'learning_rate': 0.09855988569133031, 'subsample': 0.7000692971979823, 'colsample_bytree': 0.9914280076289943, 'reg_alpha': 0.8971207068496274, 'reg_lambda': 0.9206149086096961, 'random_state': 42, 'n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_processed, y_train_reg,\n",
    "        eval_set=[(X_val_processed, y_val_reg)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_val_processed)\n",
    "    mae = mean_absolute_error(y_val_reg, preds)\n",
    "    return mae\n",
    "\n",
    "# Run the Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "# Best params â†’ use for final model\n",
    "best_params = study.best_params\n",
    "best_params[\"random_state\"] = 42\n",
    "best_params[\"n_jobs\"] = -1\n",
    "\n",
    "print(\"âœ… Best Params:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef7449",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac3b0799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Train Set Metrics:\n",
      "   - MAE:     1610.01\n",
      "   - MedAE:   591.08\n",
      "   - RMSE:    3711.72\n",
      "   - RÂ²:      0.9140\n",
      "   - MSLE:    2.2770\n",
      "\n",
      "ðŸ“Š Validation Set Metrics:\n",
      "   - MAE:     1785.54\n",
      "   - MedAE:   593.05\n",
      "   - RMSE:    4509.92\n",
      "   - RÂ²:      0.8632\n",
      "   - MSLE:    2.2240\n",
      "\n",
      "ðŸ“Š Test Set Metrics:\n",
      "   - MAE:     1768.96\n",
      "   - MedAE:   611.50\n",
      "   - RMSE:    4515.27\n",
      "   - RÂ²:      0.8775\n",
      "   - MSLE:    2.3754\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Step 1: Initialize the model (baseline config)\n",
    "xgb_model = XGBRegressor(**best_params)\n",
    "\n",
    "\n",
    "# Step 2: Train on training set\n",
    "xgb_model.fit(X_train_processed, y_train_reg)\n",
    "\n",
    "# Step 3: Predict on all sets\n",
    "y_train_pred_xgb = xgb_model.predict(X_train_processed)\n",
    "y_val_pred_xgb   = xgb_model.predict(X_val_processed)\n",
    "y_test_pred_xgb  = xgb_model.predict(X_test_processed)\n",
    "\n",
    "# Step 4: Evaluate with metrics (no plots)\n",
    "evaluate_regression_metrics(y_train_reg, y_train_pred_xgb, label=\"Train\")\n",
    "evaluate_regression_metrics(y_val_reg, y_val_pred_xgb, label=\"Validation\")\n",
    "evaluate_regression_metrics(y_test_reg, y_test_pred_xgb, label=\"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279a456",
   "metadata": {},
   "source": [
    "### Tied ensemble but got me worse results in a more training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a44f30f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.ensemble import StackingRegressor\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom xgboost import XGBRegressor\\n\\n# Base learners including the final Optuna-tuned XGBoost\\nbase_learners = [\\n    (\"xgb\", XGBRegressor(**best_params)),\\n    (\"rf\", RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\\n    (\"lr\", LinearRegression())\\n]\\n\\n# Meta-learner (can be LinearRegression or Ridge)\\nmeta_learner = LinearRegression()\\n\\n# Build stacking ensemble\\nstacked_model = StackingRegressor(\\n    estimators=base_learners,\\n    final_estimator=meta_learner,\\n    passthrough=True,  # allows meta-learner to access original features\\n    n_jobs=-1\\n)\\n\\n# Train ensemble\\nstacked_model.fit(X_train_processed, y_train_reg)\\n\\n# Predict\\ny_train_pred_ens = stacked_model.predict(X_train_processed)\\ny_val_pred_ens   = stacked_model.predict(X_val_processed)\\ny_test_pred_ens  = stacked_model.predict(X_test_processed)\\n\\n# Evaluate\\nevaluate_regression_metrics(y_train_reg, y_train_pred_ens, label=\"Train Ensemble\")\\nevaluate_regression_metrics(y_val_reg, y_val_pred_ens, label=\"Validation Ensemble\")\\nevaluate_regression_metrics(y_test_reg, y_test_pred_ens, label=\"Test Ensemble\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Base learners including the final Optuna-tuned XGBoost\n",
    "base_learners = [\n",
    "    (\"xgb\", XGBRegressor(**best_params)),\n",
    "    (\"rf\", RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "    (\"lr\", LinearRegression())\n",
    "]\n",
    "\n",
    "# Meta-learner (can be LinearRegression or Ridge)\n",
    "meta_learner = LinearRegression()\n",
    "\n",
    "# Build stacking ensemble\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    passthrough=True,  # allows meta-learner to access original features\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train ensemble\n",
    "stacked_model.fit(X_train_processed, y_train_reg)\n",
    "\n",
    "# Predict\n",
    "y_train_pred_ens = stacked_model.predict(X_train_processed)\n",
    "y_val_pred_ens   = stacked_model.predict(X_val_processed)\n",
    "y_test_pred_ens  = stacked_model.predict(X_test_processed)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_regression_metrics(y_train_reg, y_train_pred_ens, label=\"Train Ensemble\")\n",
    "evaluate_regression_metrics(y_val_reg, y_val_pred_ens, label=\"Validation Ensemble\")\n",
    "evaluate_regression_metrics(y_test_reg, y_test_pred_ens, label=\"Test Ensemble\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97942d98",
   "metadata": {},
   "source": [
    "### Dumping the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6be2812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… XGBoost model, scaler, and encoder saved.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(xgb_model, \"model_regressor.pkl\")\n",
    "\n",
    "# Save the preprocessing pipeline\n",
    "joblib.dump(scaler, \"scaler_regressor.pkl\")\n",
    "joblib.dump(encoder, \"encoder_regressor.pkl\")\n",
    "\n",
    "print(\"âœ… XGBoost model, scaler, and encoder saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84686829",
   "metadata": {},
   "source": [
    "### Comparison i made for the diffrent hyper parameters and models i used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e278808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Regression results saved to 'regression_metrics_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metrics_table = pd.DataFrame({\n",
    "    \"Metric\": [\"MAE\", \"MedAE\", \"RMSE\", \"RÂ² Score\", \"MSLE\"],\n",
    "    \"XGBoost (Manual)\": [2046.14, 552.17, 6223.33, 0.7673, 2.9971],\n",
    "    \"XGBoost (2 Trials)\": [1876.63, 454.76, 5821.84, 0.7964, 0.7080],\n",
    "    \"XGBoost (10 Trials)\": [1801.91, 417.69, 5830.47, 0.7958, 1.5963],\n",
    "    \"XGBoost (50 Trials)\": [1738.37, 397.65, 5497.54, 0.8184, 1.5618],\n",
    "    \"Stacked Ensemble\": [1768.62, 480.42, 5365.67, 0.8270, 3.7242]\n",
    "})\n",
    "\n",
    "metrics_table.to_csv(\"regression_metrics_summary.csv\", index=False)\n",
    "print(\"âœ… Regression results saved to 'regression_metrics_summary.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dashboard_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
