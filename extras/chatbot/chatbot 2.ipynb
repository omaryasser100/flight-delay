{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d30c1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SECTION 1: DATA EXPLANATION\n",
      "\n",
      "This dataset provides monthly-level flight performance data across U.S. airports and airlines. Each column gives insight into the volume, type, and causes of flight delays.\n",
      "\n",
      "COLUMN EXPLANATIONS:\n",
      "\n",
      "- year: Calendar year of the data. Helps track long-term trends.\n",
      "- month: Month of the flight. Useful for seasonal pattern detection.\n",
      "- carrier_name: The airline operating the flight. Key for comparing airline performance.\n",
      "- airport_name: The destination airport. Important \n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load your summary text\n",
    "loader = TextLoader(\"data_explanation_section.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Preview sample content\n",
    "print(documents[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "371ac1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SECTION 1: DATA EXPLANATION\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Check sample chunk\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5829b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7baa47d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "#vectorstore.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "139cdab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (1024) < n_ctx_train (2048) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "import multiprocessing\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"D:\\\\python\\\\models\\\\models--TinyLlama--TinyLlama-1.1B-Chat-v1.0\\\\tinyllama-1.1b-chat-q4_K_M.gguf\",\n",
    "    n_ctx=1024,\n",
    "    max_tokens=128,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    n_batch=512,  # test 512 if possible\n",
    "    n_threads=multiprocessing.cpu_count(),\n",
    "    stop=[\"\\n\\n\", \"Question:\"],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3},search_type=\"mmr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a41f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Create a prompt that forces the model to stay grounded\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an AI assistant. Answer the question **only based on the context** below. Do not guess, do not invent follow-up questions.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "# Create QA chain with custom prompt\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type=\"stuff\",  # Stuff simply pastes the retrieved docs into the prompt\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "752e3233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Answer:\n",
      " The late_aircraft_delay: column represents the total number of delayed flights (i.e., flights that were delayed at least 15 minutes). This measure reflects the frequency of delays caused by airline issues.\n",
      "\n",
      "ðŸ” Source Excerpt:\n",
      " - security_ct: Delays from security problems (e.g., threats). Rare but critical.\n",
      "- late_aircraft_ct: Delays due to incoming late flights. Shows cascading delay effects.\n"
     ]
    }
   ],
   "source": [
    "query = \"What does the late_aircraft_delay:  column mean?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print only the result first\n",
    "print(\"âœ… Answer:\\n\", result[\"result\"])\n",
    "\n",
    "# Then, show details (source) after\n",
    "print(\"\\nðŸ” Source Excerpt:\\n\", result[\"source_documents\"][0].page_content[:300])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
